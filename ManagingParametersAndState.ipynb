{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOfyE7qTUpIP5/Mwn0IrBQN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afairley/ColaboratoryNotebooks/blob/main/ManagingParametersAndState.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYl1CJOanGDW"
      },
      "outputs": [],
      "source": [
        "#From https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/state_params.html\n",
        "# and also https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/state_params.rst?plain=1#L59\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "from jax import random\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import optax\n",
        "\n",
        "# Create some fake data and run only for one epoch for testing.\n",
        "dummy_input = jnp.ones((3, 4))\n",
        "num_epochs = 1\n",
        "\n",
        "class BiasAdderWithRunningMean(nn.Module):\n",
        "  momentum: float = 0.9\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    is_initialized = self.has_variable('batch_stats','mean')\n",
        "    mean = self.variable('batch_stats', 'mean', jnp.zeros,\n",
        "                         x.shape[1:])\n",
        "    bias = self.param('bias', lambda rng, shape: jnp.zeros(shape),\n",
        "                      x.shape[1:])\n",
        "    if is_initialized:\n",
        "      mean.value =  (self.momentum * mean.value) + \\\n",
        "       (1.0 - self.momentum) * jnp.mean(x, axis=0, keepdims=True)\n",
        "    return mean.value + bias\n",
        "def update_step(apply_fn, x, opt_state, params, state):\n",
        "    def loss(params):\n",
        "      y, updated_state = apply_fn({'params': params, **state},\n",
        "                                  x, mutable=list(state.keys()))\n",
        "      lossValue = ((x -y) ** 2).sum()\n",
        "      return lossValue, updated_state\n",
        "    (l, updated_state), grads = jax.value_and_grad(\n",
        "        loss, has_aux=True)(params)\n",
        "    updates, opt_state = tx.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return opt_state, params, updated_state\n",
        "\n",
        "model = BiasAdderWithRunningMean()\n",
        "variables = model.init(random.key(0), dummy_input)\n",
        "state, params = flax.core.pop(variables, 'params')\n",
        "del variables\n",
        "tx = optax.sgd(learning_rate=0.02)\n",
        "opt_state = tx.init(params)\n",
        "\n",
        "for _ in range(num_epochs):\n",
        "  opt_state, params, state = update_step(\n",
        "      model.apply, dummy_input, opt_state, params, state)\n",
        ""
      ]
    }
  ]
}