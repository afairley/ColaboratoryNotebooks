{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZosef/a2SuGigjKWrpo/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afairley/ColaboratoryNotebooks/blob/main/FlaxBasics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il5tqgSIv9vR",
        "outputId": "20a55d75-af14-40d3-f81b-6ad7cc4eb668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade  -q pip jax jaxlib\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git\n",
        "import jax\n",
        "from typing import Any, Callable, Sequence\n",
        "from jax import random, numpy as jnp\n",
        "import flax\n",
        "from flax import linen as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Dense(features=5)\n",
        "key1, key2 = random.split(random.key(0))\n",
        "x = random.normal(key1, (10,))\n",
        "params = model.init(key2,x)\n",
        "jax.tree_util.tree_map(lambda x: x.shape,params)\n",
        "model.apply(params,x)\n",
        "\n",
        "n_samples = 20\n",
        "x_dim = 10\n",
        "y_dim = 5\n",
        "\n",
        "nextKey = random.key(0)\n",
        "k1, k2 = random.split(nextKey)\n",
        "W = random.normal(k1, (x_dim, y_dim))\n",
        "b = random.normal(k2,(y_dim,))\n",
        "\n",
        "true_params = flax.core.freeze({'params':{'bias': b, 'kernel': W}})\n",
        "key_sample, key_noise = random.split(k1)\n",
        "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
        "y_samples = jnp.dot(x_samples,W) + b + 0.1 *\\\n",
        " random.normal(key_noise,(n_samples, y_dim))\n",
        "print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)\n",
        "print('x:', x_samples, '; y:', y_samples)"
      ],
      "metadata": {
        "id": "snFkF1n1wan7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@jax.jit\n",
        "def mean_squared_error(params, model, x_batched, y_batched):\n",
        "  def squared_error(x, y):\n",
        "    pred = model.apply(params, x)\n",
        "    return jnp.inner(y-pred, y-pred) / 2.0\n",
        "  return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)"
      ],
      "metadata": {
        "id": "BDtCnavg1oLC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.3\n",
        "print('Loss for \"true\" W, b : ', mean_squared_error(true_params, model, x_samples, y_samples))\n",
        "loss_grad_fn = jax.value_and_grad(mean_squared_error)\n",
        "\n",
        "@jax.jit\n",
        "def update_params(params, learning_rate, grads):\n",
        "  params = jax.tree_util.tree_map(\n",
        "      lambda p, g: p - learning_rate * g, params, grads)\n",
        "  return params\n",
        "print(\"Reinitializing parameters\")\n",
        "params = model.init(key2,x)\n",
        "print(\"\\nParams\\n\", params, \"\\n\")\n",
        "for i in range(1001):\n",
        "  loss_val, grads = loss_grad_fn(params, model, x_samples, y_samples)\n",
        "  params = update_params(params, learning_rate, grads)\n",
        "  if i % 10 == 0:\n",
        "    print(f'Loss step {i}:', loss_val)\n",
        "print(\"\\nParams\\n\", params, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_69ovfG2y5s",
        "outputId": "1f289ef6-7384-4f53-e801-85923667544e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss for \"true\" W, b :  0.023639789\n",
            "Reinitializing parameters\n",
            "\n",
            "Params\n",
            " {'params': {'kernel': Array([[ 2.35571519e-01, -1.71652585e-01, -4.45728786e-02,\n",
            "        -4.68043566e-01,  4.54595268e-01],\n",
            "       [-6.87736452e-01,  3.67835373e-01, -1.79262087e-01,\n",
            "         1.29276231e-01, -2.42580160e-01],\n",
            "       [ 2.02303097e-01, -2.49465615e-01,  2.74955630e-01,\n",
            "         4.73488361e-01, -1.98002517e-01],\n",
            "       [ 2.74478316e-01, -1.21369645e-01, -2.25361675e-01,\n",
            "        -4.78193641e-01, -9.63979885e-02],\n",
            "       [-6.19886033e-02, -1.72743499e-01,  2.96945305e-04,\n",
            "        -7.17593372e-01,  2.00894207e-01],\n",
            "       [-5.60321152e-01,  3.27208370e-01,  1.06281497e-01,\n",
            "         1.28758654e-01,  1.16973236e-01],\n",
            "       [ 1.82218999e-01,  1.11444063e-01, -1.62924141e-01,\n",
            "         3.24953087e-02, -1.67053342e-01],\n",
            "       [ 4.31294113e-01,  2.08004564e-01,  1.47714227e-01,\n",
            "        -8.51502866e-02, -1.26487061e-01],\n",
            "       [ 3.29497308e-01,  1.08470365e-01, -4.01340067e-01,\n",
            "         1.66956007e-01,  5.74723601e-01],\n",
            "       [-3.84744734e-01, -3.75315547e-01, -5.35782129e-02,\n",
            "        -2.51350880e-01, -4.78640765e-01]], dtype=float32), 'bias': Array([0., 0., 0., 0., 0.], dtype=float32)}} \n",
            "\n",
            "Loss step 0: 35.343876\n",
            "Loss step 10: 0.51434696\n",
            "Loss step 20: 0.11384159\n",
            "Loss step 30: 0.03932673\n",
            "Loss step 40: 0.019916212\n",
            "Loss step 50: 0.014209141\n",
            "Loss step 60: 0.012425661\n",
            "Loss step 70: 0.011850391\n",
            "Loss step 80: 0.011661777\n",
            "Loss step 90: 0.011599412\n",
            "Loss step 100: 0.0115787005\n",
            "Loss step 110: 0.011571812\n",
            "Loss step 120: 0.011569529\n",
            "Loss step 130: 0.01156875\n",
            "Loss step 140: 0.011568497\n",
            "Loss step 150: 0.011568413\n",
            "Loss step 160: 0.01156839\n",
            "Loss step 170: 0.011568387\n",
            "Loss step 180: 0.011568384\n",
            "Loss step 190: 0.011568372\n",
            "Loss step 200: 0.011568371\n",
            "Loss step 210: 0.011568369\n",
            "Loss step 220: 0.01156837\n",
            "Loss step 230: 0.01156837\n",
            "Loss step 240: 0.011568375\n",
            "Loss step 250: 0.011568376\n",
            "Loss step 260: 0.01156837\n",
            "Loss step 270: 0.01156837\n",
            "Loss step 280: 0.011568369\n",
            "Loss step 290: 0.01156837\n",
            "Loss step 300: 0.01156837\n",
            "Loss step 310: 0.01156837\n",
            "Loss step 320: 0.01156837\n",
            "Loss step 330: 0.01156837\n",
            "Loss step 340: 0.01156837\n",
            "Loss step 350: 0.01156837\n",
            "Loss step 360: 0.01156837\n",
            "Loss step 370: 0.01156837\n",
            "Loss step 380: 0.01156837\n",
            "Loss step 390: 0.01156837\n",
            "Loss step 400: 0.01156837\n",
            "Loss step 410: 0.01156837\n",
            "Loss step 420: 0.01156837\n",
            "Loss step 430: 0.01156837\n",
            "Loss step 440: 0.01156837\n",
            "Loss step 450: 0.01156837\n",
            "Loss step 460: 0.01156837\n",
            "Loss step 470: 0.01156837\n",
            "Loss step 480: 0.01156837\n",
            "Loss step 490: 0.01156837\n",
            "Loss step 500: 0.01156837\n",
            "Loss step 510: 0.01156837\n",
            "Loss step 520: 0.01156837\n",
            "Loss step 530: 0.01156837\n",
            "Loss step 540: 0.01156837\n",
            "Loss step 550: 0.01156837\n",
            "Loss step 560: 0.01156837\n",
            "Loss step 570: 0.01156837\n",
            "Loss step 580: 0.01156837\n",
            "Loss step 590: 0.01156837\n",
            "Loss step 600: 0.01156837\n",
            "Loss step 610: 0.01156837\n",
            "Loss step 620: 0.01156837\n",
            "Loss step 630: 0.01156837\n",
            "Loss step 640: 0.01156837\n",
            "Loss step 650: 0.01156837\n",
            "Loss step 660: 0.01156837\n",
            "Loss step 670: 0.01156837\n",
            "Loss step 680: 0.01156837\n",
            "Loss step 690: 0.01156837\n",
            "Loss step 700: 0.01156837\n",
            "Loss step 710: 0.01156837\n",
            "Loss step 720: 0.01156837\n",
            "Loss step 730: 0.01156837\n",
            "Loss step 740: 0.01156837\n",
            "Loss step 750: 0.01156837\n",
            "Loss step 760: 0.01156837\n",
            "Loss step 770: 0.01156837\n",
            "Loss step 780: 0.01156837\n",
            "Loss step 790: 0.01156837\n",
            "Loss step 800: 0.01156837\n",
            "Loss step 810: 0.01156837\n",
            "Loss step 820: 0.01156837\n",
            "Loss step 830: 0.01156837\n",
            "Loss step 840: 0.01156837\n",
            "Loss step 850: 0.01156837\n",
            "Loss step 860: 0.01156837\n",
            "Loss step 870: 0.01156837\n",
            "Loss step 880: 0.01156837\n",
            "Loss step 890: 0.01156837\n",
            "Loss step 900: 0.01156837\n",
            "Loss step 910: 0.01156837\n",
            "Loss step 920: 0.01156837\n",
            "Loss step 930: 0.01156837\n",
            "Loss step 940: 0.01156837\n",
            "Loss step 950: 0.01156837\n",
            "Loss step 960: 0.01156837\n",
            "Loss step 970: 0.01156837\n",
            "Loss step 980: 0.01156837\n",
            "Loss step 990: 0.01156837\n",
            "Loss step 1000: 0.01156837\n",
            "\n",
            "Params\n",
            " {'params': {'bias': Array([-1.4540124 , -2.0262275 ,  2.0806599 ,  1.2201837 , -0.99645793],      dtype=float32), 'kernel': Array([[ 1.0106655 ,  0.19014439,  0.04533754, -0.9272265 ,  0.34720477],\n",
            "       [ 1.732027  ,  0.9901305 ,  1.1662261 ,  1.102798  , -0.10575483],\n",
            "       [-1.2009128 ,  0.28837115,  1.4176372 ,  0.12073047, -1.3132594 ],\n",
            "       [-1.194495  , -0.1899313 ,  0.03379187,  1.3165966 ,  0.0799586 ],\n",
            "       [ 0.14103451,  1.3738064 , -1.316208  ,  0.5340303 , -2.2396488 ],\n",
            "       [ 0.5643062 ,  0.8136104 ,  0.31888488,  0.53592736,  0.90351397],\n",
            "       [-0.3794808 ,  1.7408438 ,  1.0788054 , -0.5041857 ,  0.9286824 ],\n",
            "       [ 0.97013855, -1.3158665 ,  0.3363087 ,  0.80941224, -1.2024579 ],\n",
            "       [ 1.0198247 , -0.6198279 ,  1.0822716 , -1.8385581 , -0.45790648],\n",
            "       [-0.64384246,  0.45649222, -1.133104  , -0.6855652 ,  0.17010558]],      dtype=float32)}} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optax\n",
        "tx = optax.adam(learning_rate=learning_rate)\n",
        "print(\"Reinitializing parameters\")\n",
        "params = model.init(key2,x)\n",
        "opt_state = tx.init(params)\n",
        "loss_grad_fn = jax.value_and_grad(mean_squared_error)#this isn't actually needed\n",
        "\n",
        "for i in range(1001):\n",
        "  loss_val, grads = loss_grad_fn(params, model, x_samples, y_samples)\n",
        "  updates, opt_state = tx.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  if i % 10 == 0:\n",
        "    print('Loss step {}: '.format(i), loss_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbTVD8ME66Wl",
        "outputId": "26463f9c-0baa-4279-fc48-f1d6268656ed"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reinitializing parameters\n",
            "Loss step 0:  35.343876\n",
            "Loss step 10:  4.3842506\n",
            "Loss step 20:  1.277389\n",
            "Loss step 30:  0.3606207\n",
            "Loss step 40:  0.16952783\n",
            "Loss step 50:  0.080962725\n",
            "Loss step 60:  0.03698138\n",
            "Loss step 70:  0.019477855\n",
            "Loss step 80:  0.014700154\n",
            "Loss step 90:  0.01265764\n",
            "Loss step 100:  0.011839735\n",
            "Loss step 110:  0.011675058\n",
            "Loss step 120:  0.011621475\n",
            "Loss step 130:  0.011586293\n",
            "Loss step 140:  0.011573903\n",
            "Loss step 150:  0.011570497\n",
            "Loss step 160:  0.011568826\n",
            "Loss step 170:  0.011568704\n",
            "Loss step 180:  0.011568473\n",
            "Loss step 190:  0.0115683945\n",
            "Loss step 200:  0.011568376\n",
            "Loss step 210:  0.01156837\n",
            "Loss step 220:  0.011568373\n",
            "Loss step 230:  0.011568373\n",
            "Loss step 240:  0.01156837\n",
            "Loss step 250:  0.011568378\n",
            "Loss step 260:  0.0115683675\n",
            "Loss step 270:  0.011568369\n",
            "Loss step 280:  0.011568366\n",
            "Loss step 290:  0.011568363\n",
            "Loss step 300:  0.011568369\n",
            "Loss step 310:  0.011568374\n",
            "Loss step 320:  0.011568375\n",
            "Loss step 330:  0.011568363\n",
            "Loss step 340:  0.01156837\n",
            "Loss step 350:  0.011568375\n",
            "Loss step 360:  0.01156837\n",
            "Loss step 370:  0.01156837\n",
            "Loss step 380:  0.011568376\n",
            "Loss step 390:  0.011568377\n",
            "Loss step 400:  0.011568374\n",
            "Loss step 410:  0.011568365\n",
            "Loss step 420:  0.011568372\n",
            "Loss step 430:  0.01156836\n",
            "Loss step 440:  0.011568369\n",
            "Loss step 450:  0.011568368\n",
            "Loss step 460:  0.011568367\n",
            "Loss step 470:  0.011568371\n",
            "Loss step 480:  0.011568373\n",
            "Loss step 490:  0.011568378\n",
            "Loss step 500:  0.011568374\n",
            "Loss step 510:  0.011568387\n",
            "Loss step 520:  0.011568365\n",
            "Loss step 530:  0.011568376\n",
            "Loss step 540:  0.0115683805\n",
            "Loss step 550:  0.011568378\n",
            "Loss step 560:  0.01156837\n",
            "Loss step 570:  0.01156837\n",
            "Loss step 580:  0.011568365\n",
            "Loss step 590:  0.011568374\n",
            "Loss step 600:  0.01156837\n",
            "Loss step 610:  0.011568368\n",
            "Loss step 620:  0.011568373\n",
            "Loss step 630:  0.011568377\n",
            "Loss step 640:  0.011568385\n",
            "Loss step 650:  0.011568404\n",
            "Loss step 660:  0.011587354\n",
            "Loss step 670:  0.012033108\n",
            "Loss step 680:  0.011568585\n",
            "Loss step 690:  0.011720817\n",
            "Loss step 700:  0.0116038015\n",
            "Loss step 710:  0.011569034\n",
            "Loss step 720:  0.011573867\n",
            "Loss step 730:  0.011571437\n",
            "Loss step 740:  0.011569501\n",
            "Loss step 750:  0.011568782\n",
            "Loss step 760:  0.011568515\n",
            "Loss step 770:  0.011568388\n",
            "Loss step 780:  0.011568378\n",
            "Loss step 790:  0.011568381\n",
            "Loss step 800:  0.011568374\n",
            "Loss step 810:  0.011568705\n",
            "Loss step 820:  0.011699284\n",
            "Loss step 830:  0.011689692\n",
            "Loss step 840:  0.011578124\n",
            "Loss step 850:  0.011658076\n",
            "Loss step 860:  0.011676075\n",
            "Loss step 870:  0.011616535\n",
            "Loss step 880:  0.011585075\n",
            "Loss step 890:  0.011579468\n",
            "Loss step 900:  0.013530311\n",
            "Loss step 910:  0.011697653\n",
            "Loss step 920:  0.011575902\n",
            "Loss step 930:  0.01157464\n",
            "Loss step 940:  0.01157789\n",
            "Loss step 950:  0.011572386\n",
            "Loss step 960:  0.01156888\n",
            "Loss step 970:  0.01156846\n",
            "Loss step 980:  0.011568963\n",
            "Loss step 990:  0.011568675\n",
            "Loss step 1000:  0.011568371\n"
          ]
        }
      ]
    }
  ]
}